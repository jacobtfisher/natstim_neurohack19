{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare detected face locations in different pliers face detection methods\n",
    "\n",
    "\n",
    "### Tools that detect faces:\n",
    "\n",
    "* Google Cloud Vision API\n",
    "* Clarifai\n",
    "* pliers itself\n",
    "\n",
    "\n",
    "### Common measures\n",
    "* boundaries of faces\n",
    "\n",
    "\n",
    "### Ways to assess similarity\n",
    "* Eucledian distance between coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "from pliers.extractors import (ClarifaiAPIImageExtractor,\n",
    "                               FaceRecognitionFaceLocationsExtractor, \n",
    "                               GoogleVisionAPIFaceExtractor,\n",
    "                               merge_results)\n",
    "from pliers.stimuli import ImageStim\n",
    "from pliers.filters import FrameSamplingFilter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundingBox(img, coords, savename='', title=''):\n",
    "    \"\"\"\n",
    "    Plots face bounding boxes on image.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    fix,ax : figure and axis for resulting plot\n",
    "    \n",
    "    \"\"\"\n",
    "    # Plot image\n",
    "    fig,ax = plt.subplots(1)\n",
    "    if isinstance(img, str):\n",
    "        img = mpimg.imread(img)\n",
    "    imgplot = ax.imshow(img)\n",
    "\n",
    "    # For each api, add bounding boxes\n",
    "    allowed_api = ['builtin', 'clarifai', 'google']\n",
    "    api_colors = {'builtin': 'r', 'clarifai': 'c', 'google': 'b'}\n",
    "    patch_list = []\n",
    "    for api in coords.keys():\n",
    "        if api not in allowed_api:\n",
    "            raise ValueError(f'expected api specification from on in {allowed_api}, however I got \"{api}\".') \n",
    "        \n",
    "        api_coords = coords[api]   \n",
    "        for c in api_coords:\n",
    "            rect = patches.Rectangle((c[0], c[1]), c[2], c[3],\n",
    "                                 linewidth=2,\n",
    "                                 edgecolor=api_colors[api],\n",
    "                                 facecolor='none',\n",
    "                                )\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        # for legend\n",
    "        patch = patches.Patch(color=api_colors[api], label=api)\n",
    "        patch_list.append(patch)\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', handles=patch_list)\n",
    "\n",
    "    # save\n",
    "    if savename:\n",
    "        plt.savefig(savename)\n",
    "    \n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bounding(results,\n",
    "                    api='builtin',\n",
    "                    stim = 'image', \n",
    "                    x=None,\n",
    "                    y=None):\n",
    "    \"\"\"\n",
    "    Extract bounding box coordinates from a face extraction with pliers build-in tool\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    results: pandas dataframe, result of a .to_df() operation on an extraction result\n",
    "    api: one of 'builtin', 'clarifai', 'google'\n",
    "    stim: one of 'image' or 'video'\n",
    "    x, y: stimulus dimensions in pixel\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    coords: dictionary, with one key per face and coordinates in pixel. Order of coords:\n",
    "            left, bottom, width, height (how matplotlib plots it)\n",
    "    >>> extract_bounding(result_clarifai, api='clarifai', stimulus='image' x=444, y=600)\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_wh(top, right, bottom, left):\n",
    "        box_width = right - left\n",
    "        box_height = top - bottom\n",
    "        return box_width, box_height\n",
    "    \n",
    "    allowed_api = ['builtin', 'clarifai', 'google']\n",
    "    if api not in allowed_api:\n",
    "        raise ValueError(f'expected api specification from on in {allowed_api}, however I got \"{api}\".') \n",
    "    \n",
    "    # initialize an exmpty dict\n",
    "    coords = {}\n",
    "    if api == 'builtin':\n",
    "        if stim == 'image':\n",
    "            assert x, y != None\n",
    "            coords['faces'] = []\n",
    "            for idx, i in results.iterrows():\n",
    "                top, right, bottom, left = i['face_locations']\n",
    "                box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                coords['faces'].append([left, bottom, box_width, box_height])\n",
    "        elif stim == 'video':\n",
    "            # return a dictionary of dictionaries, and index appropriately\n",
    "            for idx, i in results.iterrows():\n",
    "                f = i['stim_name']\n",
    "                if f not in coords.keys(): \n",
    "                    # initialize a list to append to\n",
    "                    coords[f] = []\n",
    "                    top, right, bottom, left = i['FaceRecognitionFaceLocationsExtractor#face_locations']\n",
    "                    box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                    coords[f].append([left, bottom, box_width, box_height])\n",
    "                else:\n",
    "                    top, right, bottom, left = i['FaceRecognitionFaceLocationsExtractor#face_locations']\n",
    "                    box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                    # add to existing frame layer\n",
    "                    coords[f].append([left, bottom, box_width, box_height])                    \n",
    "\n",
    "    # Let's deal with Clarifai\n",
    "    elif api == 'clarifai':\n",
    "        if stim == 'image':\n",
    "            assert x, y != None\n",
    "            coords['faces'] = []\n",
    "            for idx, i in results.iterrows():\n",
    "                    top, right, bottom, left = i['top_row'] * y, i['right_col'] * x, i['bottom_row'] * y, i['left_col'] * x\n",
    "                    box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                    # extract coordinates and scale them to pixels\n",
    "                    coords['faces'].append([left, bottom, box_width, box_height])\n",
    "                \n",
    "                \n",
    "        elif stim == 'video':\n",
    "            # return a dictionary of dictionaries, and index appropriately\n",
    "            for idx, i in results.iterrows():\n",
    "                f = i['stim_name']\n",
    "                if f not in coords.keys():\n",
    "                    # initialize a list to append to\n",
    "                    coords[f] = []\n",
    "                    top, right, bottom, left = i['ClarifaiAPIImageExtractor#top_row'] * y, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#right_col'] * x, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#bottom_row'] * y, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#left_col'] * x\n",
    "                    box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                    coords[f].append([left, bottom, box_width, box_height])\n",
    "                else:\n",
    "                    top, right, bottom, left = i['ClarifaiAPIImageExtractor#top_row'] * y, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#right_col'] * x, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#bottom_row'] * y, \\\n",
    "                                                    i['ClarifaiAPIImageExtractor#left_col'] * x\n",
    "                    box_width, box_height = get_wh(top, right, bottom, left)\n",
    "                    # add to existing frame layer\n",
    "                    coords[f].append([left, bottom, box_width, box_height])\n",
    "                    \n",
    "    # Let's deal with Google            \n",
    "    elif api == 'google':\n",
    "        if stim == 'image':\n",
    "            coords['faces'] = []\n",
    "            for idx, i in results.iterrows():\n",
    "                co = [i['fdBoundingPoly_vertex4_x'], # bottom_left_x\n",
    "                      i['fdBoundingPoly_vertex1_y'], # top_left_y\n",
    "                      i['fdBoundingPoly_vertex3_x'] - \\\n",
    "                           i['fdBoundingPoly_vertex4_x'], # width: bottom_right_x - bottom_left_x\n",
    "                      i['fdBoundingPoly_vertex3_y'] - \\\n",
    "                           i['fdBoundingPoly_vertex2_y'] # height: bottom_right_y - top_right_y\n",
    "                      ]\n",
    "                coords['faces'].append(co)\n",
    "        elif stim == 'video':\n",
    "            # return a dictionary of dictionaries, and index appropriately\n",
    "            for idx, i in results.iterrows():\n",
    "                f = i['stim_name']\n",
    "                if f not in coords.keys():\n",
    "                    # initialize a list to append to\n",
    "                    coords[f] = []\n",
    "                    coords[f].append([i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x'], # bottom_left_x\n",
    "                              i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_y'], # top_left_y\n",
    "                              i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_x'] - \\\n",
    "                                   i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x'], # width: bottom_right_x - bottom_left_x\n",
    "                              i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_y'] - \\\n",
    "                                   i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_y'] # height: bottom_right_y - top_right_y\n",
    "                          ])\n",
    "                else:\n",
    "                    # add to existing frame layer\n",
    "                    coords[f].append([i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x'], # bottom_left_x\n",
    "                               i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_y'], # top_left_y\n",
    "                               i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_x'] - \\\n",
    "                                   i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x'], # width: bottom_right_x - bottom_left_x\n",
    "                               i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_y'] - \\\n",
    "                                   i['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_y'] # height: bottom_right_y - top_right_y\n",
    "                              ])\n",
    "            \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annotatedVideo(img_list, savename, audio=None):\n",
    "    \"\"\"\n",
    "    Creates video from images with specified duration.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    img_list : list of list, with each inner list specifying a path to an image and a duration\n",
    "    savename : str, where to save movie file\n",
    "    audio : str, optional argument for adding audio to movie\n",
    "    \n",
    "    >>> img_list = [['img1.jpj', .5], ['img2.jpg', .5]]\n",
    "    >>> extract_bounding(img_list, savename='ex_video.mp4', audio='ex_aud.mp3')\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate clips\n",
    "    clips_list = []\n",
    "    for img in img_list:\n",
    "        clip = ImageClip(img[0], duration=img[1])\n",
    "        clips_list.append(clip)\n",
    "\n",
    "    # concatenate image clips\n",
    "    print('Making video...')\n",
    "\n",
    "    # Concat clips\n",
    "    newMovie = concatenate_videoclips(clips_list) \n",
    "\n",
    "    # Add audio\n",
    "    if audio:\n",
    "        audio_clip = AudioFileClip(audio)\n",
    "        newMovie = newMovie.set_audio(audio_clip)\n",
    "\n",
    "    # Write movie\n",
    "    newMovie.write_videofile(savename, fps=24)\n",
    "\n",
    "    return newMovie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_area(width, height):\n",
    "    \"\"\"\n",
    "    Helper function to compute the area of face bounding boxes.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    area: float, area of a rectangle\n",
    "    \"\"\"\n",
    "    return width * height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_area(comparison,\n",
    "                 api_order=None,\n",
    "                 stim='image'):\n",
    "    \"\"\"\n",
    "    Compute the size differences of areas of face bounding boxes from different APIs.\n",
    "    \n",
    "    params:\n",
    "    -------\n",
    "    comparison: a list of dictionaries, with each dictionary being the output of extract_bounding()\n",
    "                on a different APIs face extraction of the same stimulus\n",
    "    api_order: list of strings with the order of APIs in the results\n",
    "    stim: one of 'image', 'video'\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    d: a nested dict with keys corresponding to face indices, and nested dicts with API area similarities.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    \n",
    "    if type(comparison) is not list:\n",
    "        raise TypeError(\"Expected a list of dictionaries, however I got an input of type {}\".format(type(comparison)))\n",
    "    api_areas = []\n",
    "    for result in comparison:\n",
    "        # we iterate over the list of dictionaries\n",
    "        areas = []\n",
    "        for k, v in result.items():\n",
    "            if len(v) > 1:\n",
    "                for box in v:\n",
    "                    # print(box)\n",
    "                    # iterate over the bounding box lists in value\n",
    "                    areas.append(compute_area(abs(box[-2]), abs(box[-1])))\n",
    "                    #print(compute_area(abs(box[-2]), abs(box[-1])))\n",
    "            if len(v) == 1:\n",
    "                # width and height are the two last items of the list. TODO: maybe we want to turn this into a named tuple\n",
    "                areas.append(compute_area(v[-2], v[-1]))\n",
    "                np.asarray(areas)\n",
    "            api_areas.append(areas)\n",
    "    # this dictionary will hold size differences\n",
    "    d = {}\n",
    "    for idx_pair in itertools.combinations(list(range(len(api_areas))), 2):\n",
    "        for idx in itertools.combinations(idx_pair, 2):\n",
    "            face_api1 = api_areas[idx[0]]\n",
    "            face_api2 = api_areas[idx[1]]\n",
    "            comparison = (str(api_order[idx[0]]) + '_' + str(api_order[idx[1]]))\n",
    "            assert len(face_api1) == len(face_api2)\n",
    "            for i in range(len(face_api1)):\n",
    "               # print(face_api1[i], face_api2[i])\n",
    "                size_diff = (min(face_api1[i], face_api2[i]) / max(face_api1[i], face_api2[i]))\n",
    "               # print(size_diff, comparison)\n",
    "                if not i in d.keys():\n",
    "                    d[i] = {}\n",
    "                    d[i][comparison] = size_diff\n",
    "                else:\n",
    "                    d[i][comparison] = size_diff\n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define static test images (single and many faces)\n",
    "img_pth = opj('../', 'data', 'obama.jpg')\n",
    "img_pth_many = opj('../', 'data', 'thai_people.jpg')\n",
    "stim = ImageStim(img_pth)\n",
    "stim_many = ImageStim(img_pth_many)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results of the face detection are given relative to stimulus size. Let's get the image dimensions in pixel\n",
    "y, x = stim.data.shape[:2]\n",
    "print(f'the one-face picture is {x} pixel x {y} pixel in size')\n",
    "\n",
    "y2, x2 = stim_many.data.shape[:2]\n",
    "print(f'the many-face picture is {x2} pixel x {y2} pixel in size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick overview of the pictures\n",
    "plt_img = mpimg.imread(img_pth)\n",
    "plt_img2 = mpimg.imread(img_pth_many)\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.imshow(plt_img)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(plt_img2)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pliers face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pliers = FaceRecognitionFaceLocationsExtractor()\n",
    "# for single face\n",
    "result_pliers = ext_pliers.transform(stim).to_df()\n",
    "# for many faces stimulus\n",
    "result_pliers_many = ext_pliers.transform(stim_many).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces for single and multi-face images from pliers-builtin, and plot them\n",
    "for res, im, x_dim, y_dim in [(result_pliers, img_pth, x, y), (result_pliers_many, img_pth_many, x2, y2)]:\n",
    "    d = extract_bounding(res, x=x_dim, y=y_dim)\n",
    "    for k, i in d.items():\n",
    "       # top, right, bottom, left = i\n",
    "     #   box_width = right-left\n",
    "     #   box_height = top-bottom\n",
    "      #  coords = [[left, bottom, box_width, box_height]]\n",
    "        plot_boundingBox(im, i, title='Pliers builtin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clarifai face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the clarifai extraction needs a model and an api key\n",
    "model='face' \n",
    "ext_clarifai = ClarifaiAPIImageExtractor(api_key='d53d5755b7514b87877df990f2d0bbd4',\n",
    "                                         model=model)\n",
    "result_clarifai = ext_clarifai.transform(stim).to_df()\n",
    "\n",
    "# for many faces\n",
    "result_clarifai_many = ext_clarifai.transform(stim_many).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces for single and multi-face images from pliers-builtin, and plot them\n",
    "for res, im, x_dim, y_dim in [(result_clarifai, img_pth, x, y), (result_clarifai_many, img_pth_many, x2, y2)]:\n",
    "    d = extract_bounding(res, api='clarifai', x=x_dim, y=y_dim)\n",
    "    for k, i in d.items():\n",
    "       # top, right, bottom, left = i\n",
    "       # box_width = right-left\n",
    "       # box_height = top-bottom\n",
    "       # coords = [[left, bottom, box_width, box_height]]\n",
    "        plot_boundingBox(im, i, 'Clarifai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud vision API face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/home/adina/NeuroHackademy-02c15db15c2a.json')\n",
    "#ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/Users/Mai/NeuroHackademy-02c15db15c2a.json')\n",
    "result_google_many = ext_google.transform(stim_many).to_df()\n",
    "results_google = ext_google.transform(stim).to_df()\n",
    "results_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google has \"wide\" and \"narrow\" bounding boxes. Here we get the wide bounding box\n",
    "\n",
    "#result_google.to_dict(orient='records')\n",
    "# vertex coordinates are in the same scale as the original image.\n",
    "# vertices are in order top-left, top-right, bottom-right, bottom-left.\n",
    "top_left_x = results_google['boundingPoly_vertex1_x'][0]\n",
    "top_right_x = results_google['boundingPoly_vertex2_x'][0]\n",
    "bottom_right_x = results_google['boundingPoly_vertex3_x'][0]\n",
    "bottom_left_x = results_google['boundingPoly_vertex4_x'][0]\n",
    "\n",
    "top_left_y = results_google['boundingPoly_vertex1_y'][0]\n",
    "top_right_y = results_google['boundingPoly_vertex2_y'][0]\n",
    "bottom_right_y = results_google['boundingPoly_vertex3_y'][0]\n",
    "bottom_left_y = results_google['boundingPoly_vertex4_y'][0]\n",
    "\n",
    "print(top_left_x, top_right_x, bottom_right_x, bottom_left_x)\n",
    "print(top_left_y, top_right_y, bottom_right_y, bottom_left_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot bounding on image \n",
    "box_width = bottom_right_x - bottom_left_x\n",
    "box_height =  bottom_right_y - top_right_y\n",
    "coords_google_wide = [[bottom_left_x, top_left_y, box_width, box_height]]\n",
    "\n",
    "plot_boundingBox(img_pth, coords_google_wide, title='Google: wide face bounding box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google has \"wide\" and \"narrow\" bounding boxes. Here we get the narrow bounding box\n",
    "\n",
    "# vertex coordinates are in the same scale as the original image.\n",
    "# vertices are in order top-left, top-right, bottom-right, bottom-left.\n",
    "top_left_x = results_google['fdBoundingPoly_vertex1_x'][0]\n",
    "top_right_x = results_google['fdBoundingPoly_vertex2_x'][0]\n",
    "bottom_right_x = results_google['fdBoundingPoly_vertex3_x'][0]\n",
    "bottom_left_x = results_google['fdBoundingPoly_vertex4_x'][0]\n",
    "\n",
    "top_left_y = results_google['fdBoundingPoly_vertex1_y'][0]\n",
    "top_right_y = results_google['fdBoundingPoly_vertex2_y'][0]\n",
    "bottom_right_y = results_google['fdBoundingPoly_vertex3_y'][0]\n",
    "bottom_left_y = results_google['fdBoundingPoly_vertex4_y'][0]\n",
    "\n",
    "print(top_left_x, top_right_x, bottom_right_x, bottom_left_x)\n",
    "print(top_left_y, top_right_y, bottom_right_y, bottom_left_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot bounding on image \n",
    "box_width = bottom_right_x - bottom_left_x\n",
    "box_height =  bottom_right_y - top_right_y\n",
    "coords_google_narrow = [[bottom_left_x, top_left_y, box_width, box_height]]\n",
    "plot_boundingBox(img_pth, coords_google_narrow, title = 'Google: narrow face bounding box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different face bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with looking at the coords\n",
    "print('pliers: ' + str(coords_pliers))\n",
    "print('clarifai: ' + str(coords_clarifai))\n",
    "print('google (wide): ' + str(coords_google_wide))\n",
    "print('google (narrow): ' + str(coords_google_narrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot on the same figure\n",
    "\n",
    "# Make a dictionary with coords\n",
    "face_apis = ['pliers', 'clarifai', 'google_wide', 'google_narrow']\n",
    "coord_dict = dict(zip(face_apis, [coords_pliers, coords_clarifai, coords_google_wide, coords_google_narrow]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute box areas of extracted faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pliers_multiface = extract_bounding(result_pliers_many, stim='image', x=x, y=y)\n",
    "clarifai_multiface = extract_bounding(result_clarifai_many, api='clarifai', stim='image', x=x, y=y)\n",
    "google_multiface = extract_bounding(result_google_many, api='google', stim='image', x=x, y=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_images = compare_area([pliers_multiface, clarifai_multiface, google_multiface],\n",
    "                            api_order=['builtin', 'clarifai', 'google'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.get_frame(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces in a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load video and downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video and downsample\n",
    "\n",
    "# Output dir\n",
    "out_dir = opj('../', 'output')\n",
    "\n",
    "# Path to video\n",
    "video_pth = opj('../', 'data', 'obama_speech.mp4')\n",
    "\n",
    "# Sample 2 frames per second\n",
    "sampler = FrameSamplingFilter(hertz=2)\n",
    "frames = sampler.transform(video_pth)\n",
    "\n",
    "x_vid = np.shape(frames.get_frame(0).data)[1]\n",
    "y_vid = np.shape(frames.get_frame(0).data)[0]\n",
    "print(x_vid)\n",
    "print(y_vid)"
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video face extraction with Google\n",
    "api = 'google'\n",
    "\n",
    "# Extract using google API\n",
    "ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/home/adina/NeuroHackademy-02c15db15c2a.json')\n",
    "results_google = ext_google.transform(frames)\n",
    "results_google = merge_results(results_google, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding boxes for every frame\n",
    "coords_dict_google = extract_bounding(results_google, api=api, stim='video')\n",
    "\n",
    "for i in range(2):\n",
    "    # get this frame\n",
    "    f = frames.get_frame(i)\n",
    "    f_data = f.data\n",
    "    f_name = f.name\n",
    "\n",
    "    # get coords\n",
    "    coords = coords_dict_google[f_name]\n",
    "    \n",
    "    # plot img with box and save    \n",
    "    #savename = opj(out_dir, 'img_' + str(i).zfill(3) + '_' + api + '.jpg')\n",
    "    plot_boundingBox(f_data, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_all = {'google': coords_dict_google}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":[],
   "source": [
    "### Clarifai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video face extraction with Clarifai\n",
    "api = 'clarifai'\n",
    "\n",
    "model='face' \n",
    "ext_clarifai = ClarifaiAPIImageExtractor(api_key='d53d5755b7514b87877df990f2d0bbd4',\n",
    "                                         model=model)\n",
    "results_clarifai = ext_clarifai.transform(frames)\n",
    "results_clarifai = merge_results(results_clarifai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding boxes for every frame\n",
    "coords_dict_clarifai = extract_bounding(result_clarifai, api=api, stim='video', x=x, y=y)\n",
    "\n",
    "for i in range(2):\n",
    "    # get this frame\n",
    "    f = frames.get_frame(i)\n",
    "    f_data = f.data\n",
    "    f_name = f.name\n",
    "\n",
    "    # get coords\n",
    "    coords = {api: coords_dict_clarifai[f_name]}\n",
    "    \n",
    "    # plot img with box and save    \n",
    "    #savename = opj(out_dir, 'img_' + str(i).zfill(3) + '_' + api + '.jpg')\n",
    "    plot_boundingBox(f_data, coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pliers builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video extraction with pliers\n",
    "api = 'builtin'\n",
    "ext_pliers = FaceRecognitionFaceLocationsExtractor()\n",
    "result_pliers = ext_pliers.transform(frames)\n",
    "result_pliers = merge_results(result_pliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bounding boxes for every frame\n",
    "coords_dict_builtin = extract_bounding(result_pliers, api=api, stim='video', x=x, y=y)\n",
    "\n",
    "for i in range(2):\n",
    "    # get this frame\n",
    "    f = frames.get_frame(i)\n",
    "    f_data = f.data\n",
    "    f_name = f.name\n",
    "\n",
    "    # get coords\n",
    "    coords = {api: coords_dict_builtin[f_name]}\n",
    "    \n",
    "    # plot img with box and save    \n",
    "    #savename = opj(out_dir, 'img_' + str(i).zfill(3) + '_' + api + '.jpg')\n",
    "    plot_boundingBox(f_data, coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make images annotated with all 3 APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make images\n",
    "coords_all = {'google': coords_dict_google, \n",
    "              'clarifai': coords_dict_clarifai, \n",
    "              'builtin': coords_dict_builtin\n",
    "             }\n",
    "\n",
    "include_apis = ['google', 'clarifai', 'builtin']\n",
    "\n",
    "for i in range(frames.n_frames):\n",
    "    # get this frame\n",
    "    f = frames.get_frame(i)\n",
    "    f_data = f.data\n",
    "    f_name = f.name\n",
    "\n",
    "    # get coords for included apis\n",
    "    frame_coords = {}\n",
    "    for api in include_apis:\n",
    "        frame_coords[api] = coords_all[api][f_name]\n",
    "    \n",
    "    # plot img with box and save    \n",
    "    savename = opj(out_dir, 'img_' + str(i).zfill(3) + '_allAPIs.jpg')\n",
    "    plot_boundingBox(f_data, frame_coords, savename=savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a video with the annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of clips and durations\n",
    "img_dir = opj('../', 'output')\n",
    "img_list = []\n",
    "for i in range(18):\n",
    "    img_list.append([opj(img_dir,'img_'+str(i).zfill(3)+'_allAPIs.jpg'), .5 ])\n",
    "\n",
    "savename = opj(out_dir, 'obama_speech_annotated.mp4')\n",
    "audio = '../data/obama_speech_audio.mp3'\n",
    "movie = write_annotatedVideo(img_list, savename=savename, audio=audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and play movie\n",
    "ipython_display(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute box area for video results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work yet\n",
    "\n",
    "google_video = extract_bounding(results_google, stim='video', api='google')\n",
    "clarifai_video = extract_bounding(results_clarifai, stim='video', api='clarifai', x=x_vid, y=y_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_area([test_res_g, test_res_a], \n",
    "             api_order=['google', 'clarifai'],\n",
    "             stim='video')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
