{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare detected face locations in different pliers face detection methods\n",
    "\n",
    "\n",
    "### Tools that detect faces:\n",
    "\n",
    "* Google Cloud Vision API\n",
    "* Clarifai\n",
    "* pliers itself\n",
    "\n",
    "\n",
    "### Common measures\n",
    "* boundaries of faces\n",
    "\n",
    "\n",
    "### Ways to assess similarity\n",
    "* Eucledian distance between coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "from pliers.extractors import (ClarifaiAPIImageExtractor,\n",
    "                               FaceRecognitionFaceLocationsExtractor, \n",
    "                               GoogleVisionAPIFaceExtractor,\n",
    "                               merge_results)\n",
    "from pliers.stimuli import ImageStim\n",
    "from pliers.filters import FrameSamplingFilter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundingBox(img, rect_coords, savename='', title=''):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    \n",
    "    if isinstance(img, str):\n",
    "        img = mpimg.imread(img)\n",
    "    \n",
    "    imgplot = ax.imshow(img)\n",
    "\n",
    "    # add bounding boxes\n",
    "    for c in rect_coords:\n",
    "        rect = patches.Rectangle((c[0], c[1]), c[2], c[3],\n",
    "                             linewidth=2,\n",
    "                             edgecolor='r',\n",
    "                             facecolor='none',\n",
    "                            )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # turn off axis    \n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    # save\n",
    "    if not savename:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awful hardcoded path to a test image\n",
    "img_pth = opj('../', 'data', 'obama.jpg')\n",
    "stim = ImageStim(img_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results of the face detection are given relative to stimulus size. Let's get the image dimensions in pixel\n",
    "y, x = stim.data.shape[:2]\n",
    "print(f'the picture is {x} pixel x {y} pixel in size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also see what the picture looks like\n",
    "fig,ax = plt.subplots(1)\n",
    "plt_img = mpimg.imread(img_pth)\n",
    "imgplot = ax.imshow(plt_img)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pliers face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pliers = FaceRecognitionFaceLocationsExtractor()\n",
    "result_pliers = ext_pliers.transform(stim).to_df()\n",
    "top, right, bottom, left = result_pliers['face_locations'][0]\n",
    "print(top, right, bottom, left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bounding on image \n",
    "coords_pliers = [[left, top, box_width, box_height]]\n",
    "plot_boundingBox(img_pth, coords_pliers, title='Pliers builtin: wide face bounding box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clarifai face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the clarifai extraction needs a model and an api key\n",
    "model='face' \n",
    "ext_clarifai = ClarifaiAPIImageExtractor(api_key='d53d5755b7514b87877df990f2d0bbd4',\n",
    "                                         model=model)\n",
    "result_clarifai = ext_clarifai.transform(stim).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_clarifai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform relative coordinates into pixel\n",
    "top_row = y * result_clarifai['top_row'][0]\n",
    "bottom_row = y * result_clarifai['bottom_row'][0]\n",
    "left_col = x * result_clarifai['left_col'][0]\n",
    "right_col = x * result_clarifai['right_col'][0]\n",
    "print(top_row, right_col, bottom_row, left_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bounding on image \n",
    "box_width = right_col - left_col\n",
    "box_height = bottom_row - top_row\n",
    "coords_clarifai = [[left_col, top_row, box_width, box_height]]\n",
    "plot_boundingBox(img_pth, coords_clarifai, title='Clarifai:  face bounding box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud vision API face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/home/adina/NeuroHackademy-02c15db15c2a.json')\n",
    "ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/Users/Mai/NeuroHackademy-02c15db15c2a.json')\n",
    "result_google = ext_google.transform(stim).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google has \"wide\" and \"narrow\" bounding boxes. Here we get the wide bounding box\n",
    "\n",
    "result_google.to_dict(orient='records')\n",
    "# vertex coordinates are in the same scale as the original image.\n",
    "# vertices are in order top-left, top-right, bottom-right, bottom-left.\n",
    "top_left_x = result_google['boundingPoly_vertex1_x'][0]\n",
    "top_right_x = result_google['boundingPoly_vertex2_x'][0]\n",
    "bottom_right_x = result_google['boundingPoly_vertex3_x'][0]\n",
    "bottom_left_x = result_google['boundingPoly_vertex4_x'][0]\n",
    "\n",
    "top_left_y = result_google['boundingPoly_vertex1_y'][0]\n",
    "top_right_y = result_google['boundingPoly_vertex2_y'][0]\n",
    "bottom_right_y = result_google['boundingPoly_vertex3_y'][0]\n",
    "bottom_left_y = result_google['boundingPoly_vertex4_y'][0]\n",
    "\n",
    "print(top_left_x, top_right_x, bottom_right_x, bottom_left_x)\n",
    "print(top_left_y, top_right_y, bottom_right_y, bottom_left_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot bounding on image \n",
    "box_width = bottom_right_x - bottom_left_x\n",
    "box_height =  bottom_right_y - top_right_y\n",
    "coords_google_wide = [[bottom_left_x, top_left_y, box_width, box_height]]\n",
    "\n",
    "plot_boundingBox(img_pth, coords_google_wide, title='Google: wide face bounding box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_google_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google has \"wide\" and \"narrow\" bounding boxes. Here we get the narrow bounding box\n",
    "\n",
    "result_google.to_dict(orient='records')\n",
    "# vertex coordinates are in the same scale as the original image.\n",
    "# vertices are in order top-left, top-right, bottom-right, bottom-left.\n",
    "top_left_x = result_google['fdBoundingPoly_vertex1_x'][0]\n",
    "top_right_x = result_google['fdBoundingPoly_vertex2_x'][0]\n",
    "bottom_right_x = result_google['fdBoundingPoly_vertex3_x'][0]\n",
    "bottom_left_x = result_google['fdBoundingPoly_vertex4_x'][0]\n",
    "\n",
    "top_left_y = result_google['fdBoundingPoly_vertex1_y'][0]\n",
    "top_right_y = result_google['fdBoundingPoly_vertex2_y'][0]\n",
    "bottom_right_y = result_google['fdBoundingPoly_vertex3_y'][0]\n",
    "bottom_left_y = result_google['fdBoundingPoly_vertex4_y'][0]\n",
    "\n",
    "print(top_left_x, top_right_x, bottom_right_x, bottom_left_x)\n",
    "print(top_left_y, top_right_y, bottom_right_y, bottom_left_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot bounding on image \n",
    "box_width = bottom_right_x - bottom_left_x\n",
    "box_height =  bottom_right_y - top_right_y\n",
    "coords_google_narrow = [[bottom_left_x, top_left_y, box_width, box_height]]\n",
    "plot_boundingBox(img_pth, coords_google_narrow, title = 'Google: narrow face bounding box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different face bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with looking at the coords\n",
    "print('pliers: ' + str(coords_pliers))\n",
    "print('clarifai: ' + str(coords_clarifai))\n",
    "print('google (wide): ' + str(coords_google_wide))\n",
    "print('google (narrow): ' + str(coords_google_narrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot on the same figure\n",
    "\n",
    "# Make a dictionary with coords\n",
    "face_apis = ['pliers', 'clarifai', 'google_wide', 'google_narrow']\n",
    "coord_dict = dict(zip(face_apis, [coords_pliers, coords_clarifai, coords_google_wide, coords_google_narrow]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to video\n",
    "video_pth = opj('../', 'data', 'obama_speech.mp4')\n",
    "\n",
    "# Sample 2 frames per second\n",
    "sampler = FrameSamplingFilter(hertz=2)\n",
    "frames = sampler.transform(video_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract using google API\n",
    "ext_google = GoogleVisionAPIFaceExtractor(discovery_file='/Users/Mai/NeuroHackademy-02c15db15c2a.json')\n",
    "results_google = ext_google.transform(frames)\n",
    "results_google = merge_results(results_google, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates of the bounding box\n",
    "# top_left_x = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_x']\n",
    "# top_right_x = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_x']\n",
    "# bottom_left_x = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_x']\n",
    "# bottom_right_x = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x']\n",
    "\n",
    "# top_left_y = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_y']\n",
    "# top_right_y = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_y']\n",
    "# bottom_left_y = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_y']\n",
    "# bottom_right_y = results_google['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in frames.n_frames:\n",
    "out_dir = opj('../', 'output')\n",
    "\n",
    "for i in range(frames.n_frames):\n",
    "    # get this frame\n",
    "    f = frames.get_frame(i)\n",
    "    f_data = f.data\n",
    "    f_name = f.name\n",
    "\n",
    "    # get api output for this frame\n",
    "    f_results_google = results_google.loc[df['stim_name'] == f_name]\n",
    "\n",
    "    # get bounding box\n",
    "    coords = []\n",
    "    for index, row in f_results_google.iterrows():\n",
    "        coords.append(get_faceBounds_google(row))\n",
    "\n",
    "    # plot img with box and save    \n",
    "    savename = opj(out_dir, 'img_' + str(i) + '.jpg')\n",
    "    plot_boundingBox(f_data, coords, savename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'b'\n",
    "if not s:\n",
    "    print('b')\n",
    "else:\n",
    "    print('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faceBounds_google(df, boxtype='tight'):\n",
    "    if boxtype is 'tight':    \n",
    "        top_left_x = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_x']\n",
    "        top_right_x = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_x']\n",
    "        bottom_left_x = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_x']\n",
    "        bottom_right_x = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_x']\n",
    "\n",
    "        top_left_y = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex1_y']\n",
    "        top_right_y = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex2_y']\n",
    "        bottom_left_y = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex3_y']\n",
    "        bottom_right_y = df['GoogleVisionAPIFaceExtractor#fdBoundingPoly_vertex4_y']\n",
    "        \n",
    "    elif boxtype is 'wide':\n",
    "        top_left_x = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex1_x']\n",
    "        top_right_x = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex2_x']\n",
    "        bottom_left_x = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex3_x']\n",
    "        bottom_right_x = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex4_x']\n",
    "\n",
    "        top_left_y = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex1_y']\n",
    "        top_right_y = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex2_y']\n",
    "        bottom_left_y = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex3_y']\n",
    "        bottom_right_y = df['GoogleVisionAPIFaceExtractor#boundingPoly_vertex4_y']\n",
    "    \n",
    "    coords = [bottom_left_x, bottom_left_y, bottom_right_x - bottom_left_x, top_right_y - bottom_right_y]\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_results_google.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_left_x)\n",
    "frames.get_frame(10).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_faces = np.unique(results_google['object_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(results_google['object_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_face1 = results_google.loc[df['object_id'] == 0]\n",
    "results_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
